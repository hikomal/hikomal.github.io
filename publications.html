---
layout: diy
title: Publications
---

<head>
<style>
a { text-decoration : none; }
a:hover { text-decoration : underline; }
a, a:visited { color : #6e6f71; }
p { font-size : 16px; }
h3 { font-size : 18px; margin : 8; padding : 0; }
.container { width : 1000px;}
.publogo { width: 100 px; margin-right : 20px; float : left; border : 10px;}
.publication { clear : left; padding-bottom : 0px; }
.publication p { height : 180px; padding-top : 0px;}
.publication strong { font-size : 17px; color : #990036; }
.publication strong a { font-size : 17px; color : #990036; }
</style>
</head>

<!--<div class="container">

<!-- <a href="https://scholar.google.com/citations?user=SL--7UMAAAAJ&hl=en" target="_blank">Google Scholar</a>
<br> -->
<!--<font color="grey" size="3">
  * Equal contribution. ♡ Project lead. ✉ corresponding / co-corresponding author.
</font>


<h3>2023</h3>

<div class="publication">
  <img src="../static/pubs/SCW23.png" class="publogo" width="200 px" height="160 px">
  <p> 
    <strong>
    MovieChat: From Dense Token to Sparse Memory in Long Video Understanding
    </strong>
    <br>
    <b>Enxin Song*</b>, Wenhao Chai*♡, Guanhong Wang*,Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Tian Ye, Jenq-Neng Hwang, Gaoang Wang✉
    <br>
    <em>Computer Vision and Pattern Recognition (CVPR), 2024</em>
    <br>
    <a href="https://rese1f.github.io/MovieChat/">[Website]</a>
    <a href="">[Paper]</a>
    <a href="">[Dataset]</a>
    <a href="https://github.com/rese1f/MovieChat">[Code]</a>
    <img alt="NPM" src="https://img.shields.io/github/stars/rese1f/MovieChat?style=social">
    <br>
    <font color="grey" size="2">
    MovieChat achieves state-of-the-art performace in long video understanding by introducing memory mechanism.
    </font>
  </p>
</div>

<div class="publication">
  <img src="../static/pubs/XXC23.png" class="publogo" width="200 px" height="140 px">
  <p> 
    <strong>
    Devil in the Number: Towards Robust Multi-modality Data Filter
    </strong>
    <br>
    Yichen Xu, Zihan Xu, Wenhao Chai*♡, Zhonghan Zhao, <b>Enxin Song</b>, Gaoang Wang✉
    <br>
    <font color="#E89B00">
    <em>International Conference on Computer Vision Workshop (ICCVW), 2023</em>
    </font>
    <br>
    <a href="https://arxiv.org/abs/2309.13770">[Paper]</a>
    <br>
    <font color="grey" size="2">
    we show that CLIP model is not robust regarding the number.
    </font>
  </p>
</div>

 <br>
 <h3>2022</h3>

 <div class="publication">
  <img src="../static/pubs/KGR22.jpg" class="publogo" width="200 px" height="140 px">
  <p> 
    <strong>
    Knowledge Graph Extrapolation Network with Transductive Learning for Recommendation
    </strong>
    <br>
    Ruixin Ma, Fangqing Guo, Liang Zhao, Biao Mei, Xiya Bu, Hao Wu, <b>Enxin Song</b>
    <br>
    <font color="#E89B00">
    <em>Applied Sciences, 2022</em>
    </font>
    <br>
    <a href="https://www.mdpi.com/2076-3417/12/10/4899">[Paper]</a>
    <br>
    <font color="grey" size="2">
    Motivated by long tail phenomenon and data sparsity, the Knowledge Graph Extrapolation Network with Transductive Learning for Recommendation is proposed to improve recommendation quality.
    </font>
  </p>
</div>
 
</div>-->
