---
layout: page
title: Wenhao (Reself) Chai
subtitle: master @UW | research intern @MSRA
use-site-title: false
---

<head>
	<style>
		a { text-decoration : none; }
		a:hover { text-decoration : underline; }
		a, a:visited { color : #b5194f; }
	</style>
	<script src="https://kit.fontawesome.com/5bef57b3e9.js" crossorigin="anonymous"></script>
</head>

<br>
Wenhao Chai is a master student at University of Washington, with <a href="https://ipl-uw.github.io/">Information Processing Lab</a> advised by Prof. <a href="https://people.ece.uw.edu/hwang/">Jenq-Neng Hwang</a>. Previously, he was an undergradate student at Zhejiang University, with 
<a href="https://cvnext.github.io/">CVNext Lab</a> advised by Prof. <a href="https://person.zju.edu.cn/en/gaoangwang/">Gaoang Wang</a>. He is fortunate to have internship at Media Computing Group, Microsoft Research Asia, advised by Dr. <a href="https://scholar.google.com/citations?user=Ow4R8-EAAAAJ&hl=en&oi=ao">Xun Guo</a>. His research interests include human pose estimation, diffusion model, video understanding, and multi-modality learning.
<br>
<br>
<i class="fa-sharp fa-solid fa-fire-flame-curved fa-beat"></i> We are hiring. I am holding a team to collaborate with <a href="https://github.com/open-mmlab">OpenMMLab</a>. See <a href="projects/index.html">Projects Page</a> for more details. Contact me (wchai@uw.edu) if you are interested.


<br>
<br>
<hr style="height:2px;border-width:0;color:gray;background-color:gray">
<b><i class="fa-regular fa-note-sticky" style="font-size:24px"></i> Selected Publications:</b>
<p><font color="grey" size="3">
Also see <a href="https://rese1f.github.io/publications" target="_blank">Publications Page</a> and <a href="https://scholar.google.com/citations?user=SL--7UMAAAAJ&hl=en" target="_blank">Google Scholar</a>.
</font></p>

<ul>
	<li>
		<p> 
			<strong>
			StableVideo: Text-driven Consistency-aware Diffusion Video Editing
			</strong>
			<br>
			<b>Wenhao Chai</b>, Xun Guo, Gaoang Wang, Yan Lu✉
			<br>
			<font color="#E89B00">
			<em>International Conference on Computer Vision (ICCV), 2023</em>
			</font>
			<br>
			<font color="grey" size="2">
			We tackle introduce temporal dependency to existing text-driven diffusion models, which allows them to generate consistent appearance for the new objects.
			</font>
		</p>
	</li>
	<li>
		<p> 
			<strong>
			Global Adaptation meets Local Generalization: Unsupervised Domain Adaptation for 3D Human Pose Estimation
			</strong>
			<br>
			<b>Wenhao Chai</b>, Zhongyu Jiang, Jenq-Neng Hwang, Gaoang Wang✉
			<br>
			<font color="#E89B00">
			<em>International Conference on Computer Vision (ICCV), 2023</em>
			</font>
			<br>
			<a href="https://arxiv.org/abs/2303.16456">[Paper]</a>
			<a href="https://github.com/rese1f/PoseDA">[Code]</a>
			<img alt="NPM" src="https://img.shields.io/github/stars/rese1f/PoseDA?style=social">
			<br>
			<font color="grey" size="2">
			A simple yet effective framework of unsupervised domain adaptation for 3D human pose estimation.
			</font>
	  	</p>
	</li>
	
</ul>

<hr style="height:2px;border-width:0;color:gray;background-color:gray">
<b><i class="fa-solid fa-pen-to-square" style="font-size:24px"></i> Updates:</b><br><br>

<ul>
	<li><i>July 2023: <img src="static/imgs/microsoft.png" width="25" height="25" style="vertical-align:text-bottom"/> Finish my internship at Microsoft Research Asia (MSRA), Beijing. I appreciate the helpful guidance and suggestions from my mentor Dr. Xun Guo during the internship.
	</li><br>

	<li><i>July 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>StableVideo: Text-driven Consistency-aware Diffusion Video Editing</i> is accepted by ICCV 2023.
	</li><br>

	<li><i>July 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Global Adaptation meets Local Generalization: Unsupervised Domain Adaptation for 3D Human Pose Estimation</i> is accepted by ICCV 2023.
	</li><br>

	<li><i>July 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>DiffFashion: Reference-based Fashion Design with Structure-aware Transfer by Diffusion Models</i> is accepted by IEEE T-MM.
	</li><br>
	
	<li><i>June 2023:</i> <img src="static/imgs/zju.png" width="25" height="25" style="vertical-align:text-bottom"/> I graduate from Zhejiang University.
	</li><br>

	<li><i>Apr 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> The short version of our paper <i>DiffFashion: Reference-based Fashion Design with Structure-aware Transfer by Diffusion Models</i> is accepted by CVPR 2023 <a href="http://conferences.visionbib.com/2023/cvpr-cvfad-6-23-call.html">6th Workshop on Computer Vision for Fashion, Art, and Design</a>.
	</li><br>

	<li><i>Mar 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Deep Learning Methods for Small Molecule Drug Discovery: A Survey</i> is accepted by IEEE T-AI.
	</li><br>

	<li><i>Mar 2023:</i> <img src="static/imgs/uw.png" width="36" height="25" style="vertical-align:text-bottom"/> Become a graduate student member of <a href="https://ipl-uw.github.io/">Information Processing Lab</a> at University of Washington, advised by Professor <a href="https://people.ece.uw.edu/hwang/">Jenq-Neng Hwang</a>.
	</li><br>
	
	<li><i>Feb 2023:</i> <img src="static/imgs/microsoft.png" width="25" height="25" style="vertical-align:text-bottom"/> Become a research intern at <a href="https://www.msra.cn/">Microsoft Research Asia (MSRA)</a>, advised by principal researcher <a href="https://scholar.google.com/citations?user=Ow4R8-EAAAAJ&hl=en&oi=ao">Xun Guo</a>.
	</li><br>

	<li><i>Oct 2022:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Automatic Spinal Ultrasound Image Segmentation and Deployment for Real-time Spine Volumetric Reconstruction</i> accepted at ICUS 2022 with best paper award.
	</li><br>

	<li><i>Sep 2022:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Weakly Supervised Two-Stage Training Scheme for Deep Video Fight Detection Model</i> accepted at ICTAI 2022. we release a new dataset, <a href="https://github.com/rese1f/VideoFightDetection">VFD-2000</a>, that specializes in video fight detection, with a larger scale and more scenarios than existing datasets.
	</li><br>

	<li><i>Aug 2022:</i> <img src="static/imgs/alibaba.png" width="36" height="25" style="vertical-align:text-bottom"/> Visit <a href="https://damo.alibaba.com">DAMO Academy</a> at Alibaba.
	</li><br>

	<li><i>July 2022:</i> <img src="static/imgs/uiuc.png" width="18" height="25" style="vertical-align:text-bottom"/> Become a member of <a href="https://www.ncsa.illinois.edu/">National Center for Supercomputing Applications (NCSA)</a> at University of Illinois Urbana-Champaign, work with Professor <a href="https://cs.illinois.edu/about/people/faculty/kindrtnk">Volodymyr (Vlad) Kindratenko</a>.
	</li><br>

	<li><i>June 2022:</i> Attend <a href="http://cvpr2022.thecvf.com/">CVPR 2022</a> in New Orleans. Here are my <a href="https://github.com/rese1f/awesome-cvpr2022">notes</a>.
	</li><br>

	<li><i>June 2022:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Deep Vision Multimodal Learning: Methodology, Benchmark, and Trend</i> accepted by Applied Science.
	</li><br>

	<li><i>July 2021:</i> Start my research on 3D human pose estimation task, advised by Professor <a href="https://person.zju.edu.cn/en/gaoangwang/">Gaoang Wang</a>.
	</li><br>

</ul>